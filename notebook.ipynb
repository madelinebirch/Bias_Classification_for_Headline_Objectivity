{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4783f0c6",
   "metadata": {},
   "source": [
    "## Gender Bias  Classification Modeling as a Foundation for Headline Objectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f53878c",
   "metadata": {},
   "source": [
    "Madeline F. Birch | November 2023 | Flatiron School Data Science Program | Final Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3765b098",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. [Project Overview](#Project_Overview)\n",
    "2. [The Dataset](#The_Dataset)\n",
    "3. [Exploratory Data Analysis](#EDA)\n",
    "4. [Feature Engineering](#Feature_Engineering)\n",
    "5. [Preprocessing](#Preprocessing)\n",
    "6. [Train Test Split](#Train_Test_Split)\n",
    "7. [Modeling](#Modeling)\n",
    "8. [Findings](#Findings)\n",
    "9. [Recommendations](#Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2de646",
   "metadata": {},
   "source": [
    "# Project Overview<a id='Project_Overview'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d55224",
   "metadata": {},
   "source": [
    "In an era where information shapes perspectives, the media plays a pivotal role in influencing societal narratives. Understanding the subtle nuances and potential biases embedded in headlines is crucial, and this project aims to shed light on the degree of bias present in headlines. Focusing on data classification through machine learning, the project seeks to predict headlines into three classes: No Bias, Low Bias, and High Bias. The objective is not to scrutinize sensationalism or analyze sentiment polarity but rather to leverage textual and numerical features to predict the bias level accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c681823",
   "metadata": {},
   "source": [
    "<img src=\"Images/pbs_logo.png\" alt=\"PBS Logo\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54bfb6",
   "metadata": {},
   "source": [
    "### Our Stakeholder: PBS News\n",
    "The significance of this project lies in its potential impact on journalism's objectivity, particularly for PBS News, a revered American news source known for its impartiality and lack of apparent agenda. Publicly funded by 15%, the entity continuously faces accusations of general bias and [threats of defunding from various actors](https://www.nytimes.com/2011/02/28/business/media/28cpb.html). \n",
    "\n",
    "By adopting insights gained from our efforts, PBS News can *silence these threats* and *secure its value* as a trusted source for objective, unbiased headlines.\n",
    "\n",
    "### Our Vision: Gender Bias in Headlines as a Framework\n",
    "We chose to focus on gender bias as a focus for this project because it is undeniably one of the most prevalent forms of bias in published news content. A 2021 Topic Modeling [study](https://www.frontiersin.org/articles/10.3389/frai.2021.664737/full) found women are unfortunately but not unexpectedly *mentioned* \"more frequently in topics related to lifestyle, healthcare, and crimes and sexual assault.\" Another 2021 Natural Language Processing [study](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0245533) concluded, \"although\n",
    "we see a certain tokenism in having female *voices* present in the news, their voices are drowned\n",
    "out by the overwhelming number of times that we hear from men, often from just a handful of\n",
    "men.\" There's no denying that gender bias is present in news article content, both in references toward and quotations credited to women, but what about the content of *headlines?* Couldn't headlines about women be biased, too?\n",
    "\n",
    "At the heart of this initiative is the recognition of headlines as powerful agents that shape our perceptions. These succinct phrases captivate our attention and mold our subconscious understanding of entire articles. The urgency to prove and maintain neutrality, especially on a platform as eminent as PBS, underscores the relevance of our undertaking.\n",
    "\n",
    "While our primary focus is on demonstrating which ML algorithms are most adept at detecting gender bias in headlines, this project could serve a framework for ongoing assessments of headlines across diverse bias types, including political, racial, LGBTQ+, socioeconomic class and beyond. The broader vision is to contribute to a media landscape characterized by transparency, objectivity, and accountability, fostering a public discourse grounded in fair and unbiased reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545fa3d",
   "metadata": {},
   "source": [
    "# The Dataset<a id='The_Dataset'></a>\n",
    "\n",
    "The dataset utilized in this project originates from a comprehensive collection of data scraped for [\"When Women Make Headlines,\"](https://pudding.cool/2022/02/women-in-headlines/) a visual essay published by [*The Pudding*](https://pudding.cool/) in June 2022. Released by [Amber Thomas](https://data.world/amberthomas) on [data.world](https://data.world/the-pudding/women-in-headlines), this dataset is one of few open source datasets we could find that investigates gender bias specifically in headlines. It encompasses a diverse array of headlines about women, each annotated with corresponding bias scores. Bias scores were calculated following the methodology outlined in [\"Proposed Taxonomy for Gender Bias in Text; A Filtering Methodology for the Gender Generalization Subtype.\"](https://aclanthology.org/W19-3802.pdf)\n",
    "\n",
    "The dataset's origin in a visual essay adds an element of real-world applicability, grounding the project in the practical considerations of media consumption and perception. Its richness lies in its amalgamation of both textual numerical features associated with each headline. The text data provides the linguistic context of the headlines, while numerical features offer additional dimensions for analysis. This holistic approach enables the development of a machine learning model that can discern patterns beyond linguistic constructs, contributing to a nuanced understanding of what contributes to bias. \n",
    "\n",
    "### Our Working File\n",
    "The dataset contains a multitude of `.csv` files; for the sake of simplicity, we will be working exclusively with `headlines.csv` and engineering additional numerical features to bolster our models.\n",
    "\n",
    "### Our Target\n",
    "Our target variable will be `bias`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03093f02",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis<a id='EDA'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724c1257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01a9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 'headlines.csv' into a Pandas DataFrame\n",
    "headlines = pd.read_csv('Data/headlines.csv')\n",
    "\n",
    "# Showing first 5 rows of the DataFrame\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16eff99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns 'url', 'Unnamed: 0', and 'index'\n",
    "headlines = headlines.drop(columns=['url', 'Unnamed: 0', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8770116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the shape of our DataFrame\n",
    "headlines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe02872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting primary statistics\n",
    "headlines.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc7e676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting value counts for bias feature\n",
    "headlines['bias'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e11c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting bias distribution\n",
    "\n",
    "# Setting Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting histogram of 'bias'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines['bias'], bins=20, kde=True, color='blue')\n",
    "\n",
    "# Styling the plot\n",
    "plt.title('Distribution of Bias Scores', fontsize=16)\n",
    "plt.xlabel('Bias Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Showing the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3f768",
   "metadata": {},
   "source": [
    "We see from our preliminary EDA that:\n",
    "- Our DataFrame is fairly large, with a shape of (382139, 5). We will need to filter the dataset down a bit given the limitations of our project.\n",
    "- The target variable 'bias' is quite imbalanced, as the majority of instances (265271) have a bias score of 0.166667.\n",
    "- Because there are only 6 distinct bias scores, we will be treating the numerical bias scores as categorical. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e00f96",
   "metadata": {},
   "source": [
    "# Feature Engineering (for more EDA and Future Implementation) <a id='Feature_Engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39deaac0",
   "metadata": {},
   "source": [
    "We will continue our EDA by plotting various distributions, engineering more features, plotting interactions, and filtering the dataset to a manageable size. The purpose of this section is to show the breadth of engineering possibilities for future implementation; including all of these added features in modeling poses a significant challenge to the scope and time constraints on this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a87961c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating feature 'sentiment_polarity'- scale of -1 to 1\n",
    "\n",
    "# Creating a sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Applying the sentiment analyzer to each headline and storing the compound score - this takes a while to run\n",
    "headlines['sentiment_polarity'] = headlines['headline_no_site'].apply(lambda x: sid.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5307a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting distribution of sentiment_polarity\n",
    "headlines['sentiment_polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d4d6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing min and max values for sentiment polarity\n",
    "print(headlines['sentiment_polarity'].min())\n",
    "print(headlines['sentiment_polarity'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "904aaa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dist of sentiment polarity\n",
    "\n",
    "# Setting Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting histogram of 'sentiment_polarity'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines['sentiment_polarity'], bins=20, kde=True, color='green')\n",
    "\n",
    "# Styling the plot\n",
    "plt.title('Distribution of Sentiment Polarity Scores', fontsize=16)\n",
    "plt.xlabel('Sentiment Polarity Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Showing the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4883de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering 'Day of the Week' and 'Month' Features\n",
    "headlines['Day_of_Week'] = pd.to_datetime(headlines['time']).dt.day_name()\n",
    "headlines['Month'] = pd.to_datetime(headlines['time']).dt.month\n",
    "\n",
    "# Engineering 'Hour of Dat' feature\n",
    "headlines['Hour_of_Day'] = pd.to_datetime(headlines['time']).dt.hour\n",
    "\n",
    "# Converting 'time' column to datetime format\n",
    "headlines['time'] = pd.to_datetime(headlines['time'], errors='coerce')\n",
    "\n",
    "# Extracting the year and creating a new 'Publication Year' feature\n",
    "headlines['Publication_Year'] = headlines['time'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96150e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping time column\n",
    "headlines = headlines.drop(columns=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab2c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word count feature\n",
    "headlines['Word_Count'] = headlines['headline_no_site'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Creating text length feature\n",
    "headlines['Text_Length'] = headlines['headline_no_site'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fd4a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting new head\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bc5ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting value counts for site\n",
    "headlines['site'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea13b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a threshold for news sites with at least 5000 headlines\n",
    "min_headlines_threshold = 5000\n",
    "top_sites = headlines['site'].value_counts()\n",
    "top_sites = top_sites[top_sites >= min_headlines_threshold].index\n",
    "\n",
    "# Creating a new dataframe with only the sites with at least 5000 headlines\n",
    "headlines_filtered = headlines[headlines['site'].isin(top_sites)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e024126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting value counts for country\n",
    "headlines_filtered['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "609bf964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting distribution of countries\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='country', data=headlines_filtered, palette='viridis')\n",
    "plt.title('Distribution of Countries')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of Headlines')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b1a5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out all data for 'UK' and 'India' so that we stay in the US\n",
    "headlines_filtered = headlines_filtered[headlines_filtered['country'].isin(['USA'])]\n",
    "\n",
    "# Verify the changes\n",
    "print(headlines_filtered['country'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9193378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting shape of new DataFrame\n",
    "headlines_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9078e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting top 10 sites with most headlines\n",
    "top_10_sites = headlines_filtered['site'].value_counts().nlargest(10)\n",
    "\n",
    "# Creating a pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(top_10_sites, labels=top_10_sites.index, autopct='%1.1f%%', colors=sns.color_palette('viridis'), startangle=90)\n",
    "plt.title('Top 10 News Sources Distribution')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef5282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b17aa72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines_filtered['Word_Count'], bins=30, color='skyblue', kde=False)\n",
    "plt.title('Distribution of Word Count in Headlines')\n",
    "plt.xlabel('Word_Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70a0fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines_filtered['Text_Length'], bins=30, color='skyblue', kde=False)\n",
    "plt.title('Distribution of Text Length in Headlines')\n",
    "plt.xlabel('Text_Length')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "111a8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines_filtered['Hour_of_Day'], bins=30, color='skyblue', kde=False)\n",
    "plt.title('Distribution of Hour of Day')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27b0862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Bias Category by Day of Week\n",
    "\n",
    "# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a count plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=headlines_filtered, x='Day_of_Week', hue='bias_category', palette='Set2')\n",
    "plt.title('Bias Category by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Bias Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d38e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting distribution of bias in headlines_filtered\n",
    "headlines_filtered['bias'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02728d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting values to drop\n",
    "values_to_drop = [0.666667, 0.833333]\n",
    "\n",
    "# Use boolean indexing to drop rows with specified values in 'bias' column\n",
    "headlines_filtered = headlines_filtered[~headlines_filtered['bias'].isin(values_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining conditions\n",
    "conditions = [\n",
    "    headlines_filtered['bias'].between(0.000000, 0.000000, inclusive='both'),\n",
    "    headlines_filtered['bias'].between(0.1, 0.2, inclusive='both'),\n",
    "    headlines_filtered['bias'].between(0.3, 0.5, inclusive='both'),\n",
    "]\n",
    "\n",
    "# Setting category labels\n",
    "labels = ['No Bias', 'Low Bias', 'High Bias']\n",
    "\n",
    "# Applying conditions\n",
    "headlines_filtered['bias_category'] = np.select(conditions, labels, default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070309f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting unique bias category values\n",
    "headlines_filtered['bias_category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered['bias_category'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca00cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered = headlines_filtered.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e0462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered['bias_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f476c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping original bias column\n",
    "headlines_filtered = headlines_filtered.drop(columns=['bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23062ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of bias_category by Publication_Year\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=headlines_filtered, x='Publication_Year', y='bias_category', ci=None)\n",
    "plt.title('Bias Category Distribution Over Publication Year')\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Bias Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Bias Category by Site\n",
    "\n",
    "# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a count plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.countplot(data=headlines_filtered, x='site', hue='bias_category', palette='viridis')\n",
    "plt.title('Bias Category by Site')\n",
    "plt.xlabel('Site')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
    "plt.legend(title='Bias Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Distribution of Sentiment Polarity by Bias Categories\n",
    "\n",
    "# Setting a Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Setting fig size, creating plot of feature interaction\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Creating a violin plot with strip plot overlay\n",
    "violin = sns.violinplot(x='bias_category', y='sentiment_polarity', data=headlines_filtered, inner='quartile', palette='viridis')\n",
    "strip = sns.stripplot(x='bias_category', y='sentiment_polarity', data=headlines_filtered, color='black', size=2, jitter=True)\n",
    "\n",
    "# Styling the plot\n",
    "plt.title('Distribution of Sentiment Polarity by Bias Categories', fontsize=16)\n",
    "plt.xlabel('Bias Category', fontsize=12)\n",
    "plt.ylabel('Sentiment Polarity', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend(title='Bias Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Showing the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ee672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the 'headlines_no_site' column to 'headlines'\n",
    "headlines_filtered.rename(columns={'headline_no_site': 'headlines'}, inplace=True)\n",
    "\n",
    "headlines_filtered['headlines'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d4815",
   "metadata": {},
   "source": [
    "The main takeaway from this section is that we have an imbalanced dataset, skewing towards the Low Bias category. Since classification models don't assume a normal distribution of the target, we won't worry too much about that. We will simply see how the models perform and attempt to explain their behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772dcbca",
   "metadata": {},
   "source": [
    "# Preprocessing<a id='Preprocessing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20e7b9",
   "metadata": {},
   "source": [
    "To sufficiently prepare our modeling, we must properly preprocess our text data. We begin by importing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfeffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e426f29",
   "metadata": {},
   "source": [
    "We proceed with **tokenizing the headline text:** The `word_tokenize` function is applied to each headline to split it into a list of words or tokens, and the result is stored in the 'tokenized_text' column.\n",
    "\n",
    "We then **remove non-alphabetic characters, handling empty strings, and extra spaces:** The 'cleaned_text' column is created by applying a lambda function to each tokenized text. This function removes non-alphabetic characters, handles empty strings, and strips extra spaces from each token.\n",
    "\n",
    "Next, we **converting to lowercase:** Another lambda function is applied to convert all words in the 'cleaned_text' column to lowercase, ensuring uniformity in text case.\n",
    "\n",
    "Finally, we proceed with **Lemmatization:** Lemmatization reduces words to their base or root form, aiding in text normalization. The 'lemmatized_text' column is created by applying a lambda function that lemmatizes each word in the 'cleaned_text' column using the WordNet lemmatizer.  Note that lemmatization can take a bit of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ea1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, Clean, and Lemmatize Text\n",
    "\n",
    "# Tokenizing the headline text\n",
    "headlines_filtered['tokenized_text'] = headlines_filtered['headlines'].apply(word_tokenize)\n",
    "\n",
    "# Removing non-alphabetic characters, handle empty strings, and extra spaces\n",
    "headlines_filtered['cleaned_text'] = headlines_filtered['tokenized_text'].apply(lambda tokens: [re.sub(r'[^a-zA-Z0-9]', '', token).strip() for token in tokens if re.sub(r'[^a-zA-Z0-9]', '', token).strip()])\n",
    "\n",
    "# Converting to lowercase\n",
    "headlines_filtered['cleaned_text'] = headlines_filtered['cleaned_text'].apply(lambda tokens: [token.lower() for token in tokens])\n",
    "\n",
    "# Lemmatization - this takes a minute or two to run\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "headlines_filtered['lemmatized_text'] = headlines_filtered['cleaned_text'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b765d14",
   "metadata": {},
   "source": [
    "Let's make a copy of `headlines_filtered`, save it as `lemmatized_df` and clean it up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing a copy of headlines_filtered_encoded as lemmatized_df\n",
    "lemmatized_df = headlines_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d498ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'headlines' column from lemmatized_df\n",
    "lemmatized_df.drop('headlines', axis=1, inplace=True)\n",
    "\n",
    "# Displaying the first row of lemmatized_df after dropping the column\n",
    "lemmatized_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d25756",
   "metadata": {},
   "source": [
    "\n",
    "We'll now proceed to eliminate stopwords, which don't hold much meaning.\n",
    "First, we **get stop words:** The `stopwords.words('english')` function retrieves a set of English stop words from NLTK's corpus and is stored in the 'stop_words' variable.\n",
    "\n",
    "Then, we can **create no stopwords feature:** A new column 'lemmatized_text_no_stopwords' is created in the 'lemmatized_df' DataFrame. It is obtained by applying a lambda function to each list of lemmatized tokens in the 'lemmatized_text' column. This function filters out tokens that are stop words, effectively removing them from the lemmatized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ffddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Creating no stopwords feature, removing stop words from the lemmatized_text column\n",
    "lemmatized_df['lemmatized_text_no_stopwords'] = lemmatized_df['lemmatized_text'].apply(lambda tokens: [token for token in tokens if token not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting head of lemmatized_text_no_stopwords feature\n",
    "lemmatized_df['lemmatized_text_no_stopwords'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98873a4e",
   "metadata": {},
   "source": [
    "Let's make a copy of our working dataframe, drop some columns, and inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b920d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing a copy of lemmatized_df as df_to_vectorize\n",
    "df_to_vectorize = lemmatized_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ef505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting list of columns to drop\n",
    "columns_to_drop = ['tokenized_text', 'cleaned_text', 'lemmatized_text']\n",
    "\n",
    "# Dropping the specified columns\n",
    "df_to_vectorize.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting head \n",
    "df_to_vectorize.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435ee16",
   "metadata": {},
   "source": [
    "The following few code blocks were adapted from a [project](https://github.com/melissaleeyvr/Predicting-Media-Bias-in-News-Articles) by Melissa Lee, available on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1d37f",
   "metadata": {},
   "source": [
    "Before vectorizing, or representing our text numerically, it would be extremely helpful if we could download a pretrained model to help us capture semantic relationships between common news words. \n",
    "\n",
    "To do this, we:\n",
    "\n",
    "**Print available models:** The `gensim.downloader.info()` function provides information about available models in `Gensim`'s `gensim-data` module. The list of model names is obtained using ['models'].keys(), and it is printed.\n",
    "\n",
    "**Download a pre-trained word embedding model:** The `gensim.downloader.load('fasttext-wiki-news-subwords-300')` function downloads a pre-trained word embedding model named 'fasttext-wiki-news-subwords-300' from gensim's model repository. The downloaded model is assigned to the variable 'model'. Note that downloading the model may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbd00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading a pre-trained word embedding model and assigning it to 'model'- this will take a while! \n",
    "model = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ad664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function\n",
    "def text2vec(text):\n",
    "        # Assigning the input list of tokenized words to the variable 'tokenized'\n",
    "    tokenized = text\n",
    "        # Initializing an empty list to store word embeddings\n",
    "    word_embeddings = [np.zeros(300)]\n",
    "        # Iterating over each word in the tokenized list\n",
    "    for word in tokenized:\n",
    "        if word in model:  # Checking if the word is present in the pre-trained word embedding model\n",
    "            vector = model[word]  # If present, retrieving the word embedding vector for the word\n",
    "        else:\n",
    "            vector = np.zeros(300) # If not present, assigning a zero vector\n",
    "\n",
    "        word_embeddings.append(vector)  # Appending the word embedding vector to the list\n",
    "    # Converting the list of word embeddings to a NumPy array and calculating the mean along the axis 0\n",
    "    text_embedding = np.stack(word_embeddings).mean(axis=0) \n",
    "    # Returning the average word embedding for the entire text\n",
    "    return text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function over the lemmatized text column and assigning the results to new column\n",
    "df_to_vectorize['headline_vectors'] = df_to_vectorize['lemmatized_text_no_stopwords'].apply(lambda x: text2vec(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting headline vectors\n",
    "df_to_vectorize['headline_vectors'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making copy of df_to_vectorize\n",
    "final_df = df_to_vectorize.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'lemmatized_text_no_stopwords' column\n",
    "final_df = final_df.drop('lemmatized_text_no_stopwords', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39858322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking headlines vectors of first row in new filtered dataset\n",
    "final_df['headline_vectors'][9207]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d4ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking final_df for null values\n",
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ea846",
   "metadata": {},
   "source": [
    "Our text data is now vectorized and ready for a train-test split!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb566d8",
   "metadata": {},
   "source": [
    "# Train Test Split<a id='Train_Test_Split'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b5101",
   "metadata": {},
   "source": [
    "We begin by importing `train_test_split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63a8b5",
   "metadata": {},
   "source": [
    "Now, we set our X and y variables to 'headline_vectors' and 'bias_category', respectively. Then, we split the data using `train_test_split` function, specifying a test size of 0.2 and a random state of 42. Then, we print the shape of each variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b9d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X and y variables\n",
    "X = final_df[['headline_vectors']]\n",
    "y = final_df['bias_category']\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Printing the shapes of each variable\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring there are no NaN values in y_train\n",
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480f8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring there are no NaN values in y_test\n",
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d42bfdd",
   "metadata": {},
   "source": [
    "Most machine learning models in libraries expect input features to be in the form of arrays, so we will have to convert our X_train and X_test variables to arrays. To do this, we: **extract the vector** and employ the `.tolist()` function, which converts the column of vectors into a Python list. This step is necessary because the subsequent conversion to a NumPy array (`np.array()`) expects an iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting X_train vectors to arrays\n",
    "X_train_array = np.array(X_train['headline_vectors'].tolist())\n",
    "\n",
    "# Converting X_test vectors to arrays\n",
    "X_test_array = np.array(X_test['headline_vectors'].tolist())                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a83e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting X_train_array\n",
    "X_train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db802c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting X_test_array\n",
    "X_test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323da41",
   "metadata": {},
   "source": [
    "Before we begin modeling, let's discuss the evaluation metric we will focus on for evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12fec0",
   "metadata": {},
   "source": [
    "## Evaluation Metric: Weighted Average F1<a id='Evaluation Metrics'></a>\n",
    "We will be focusing on Weighted Average F1 score as an evaluation metric across all models.\n",
    "\n",
    "#### Context for Our Stakeholder \n",
    "\n",
    "#### False Positives\n",
    "- **Cost**: A false positive occurs when a headline is *incorrectly classified as having a bias that it does not possess.* For PBS, false positives can lead to inaccurate categorization for otherwise passable headlines, which we want to avoid.\n",
    "- **Implication for our Stakeholder**: It is crucial to minimize false positives to maintain the credibility of PBS's ability to classify bias. Readers relying on these classifications should trust that the information accurately reflects the intended bias.\n",
    "\n",
    "#### False Negatives\n",
    "- **Cost**: A false negative occurs when a biased headline is *incorrectly classified as unbiased.* This might result in overlooking misleading information and passing off headlines that aren't fit to be published.\n",
    "- **Implication for our Stakeholder**: Striking the right balance is essential to ensure readers are alerted to potential bias when it exists.\n",
    "\n",
    "#### The Role of Weighted F1 Score\n",
    "The Weighted Average F1 score is an appropriate evaluation metric for predicting headline bias categories for PBS due to its ability to balance precision and recall while accounting for class imbalance. In the context of classifying bias categories, it ensures that the model's performance is assessed considering *both false positives and false negatives,* providing a comprehensive measure of predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd09952",
   "metadata": {},
   "source": [
    "# Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964faa1e",
   "metadata": {},
   "source": [
    "We begin our modeling process by running a few simple models as a reference point for more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27c737",
   "metadata": {},
   "source": [
    "Let's begin our by importing all necessary libraries, modules and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, f1_score, accuracy_score, precision_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb65d9",
   "metadata": {},
   "source": [
    "## Logistic Regression<a id='Logistic Regression'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2bdc3",
   "metadata": {},
   "source": [
    "Logistic Regression is a good jumping off point due to its simplicity, interpretability, and efficiency in handling binary or multiclass classification tasks. To start, we initialize a logistic regression model (`logreg_model`), and the `max_iter` parameter is set to 1000 to ensure convergence during the training process. We then fit the model to the training variables, make predictions on the test variable, and calculate the weighted average precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f037be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a logistic regression model\n",
    "logreg_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "logreg_model.fit(X_train_array, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test set\n",
    "logreg_y_pred = logreg_model.predict(X_test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd85c05d",
   "metadata": {},
   "source": [
    "Not a bad WAP score... but, to get a holistic understanding of the score, let's see what a confusion matrix and a classification report might tell us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating confusion matrix\n",
    "logreg_cm = confusion_matrix(y_test, logreg_y_pred)\n",
    "logreg_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36dc126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix using a heatmap\n",
    "sns.heatmap(logreg_cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=logreg_model.classes_, yticklabels=logreg_model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "logreg_report = classification_report(y_test, logreg_y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", logreg_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a874c",
   "metadata": {},
   "source": [
    "The model performs well in classifying \"Low Bias\" with high precision (0.69) and recall (0.97),which is to be expected, as there are far more instances of Low Bias than No or High.\n",
    "We see the model struggles with \"High Bias,\" showing lower precision (0.62) and recall (0.19). The \n",
    "\"No Bias\" category has perfect precision but lacks any recall. So, the model correctly identifies instances belonging to the \"No Bias\" category when it predicts it, but it might be missing a significant number of actual \"No Bias\" instances, showing an inability to generalize to new data.\n",
    "\n",
    "**LogReg Weighted Average F1 score:** 60%.\n",
    "\n",
    "A score 0.60 indicates a relatively good overall performance of the Logistic Regression model, though the overall imbalance in the model is suspect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da16c281",
   "metadata": {},
   "source": [
    "## Random Forest<a id='Random Forest'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac401be4",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble learning method that constructs a multitude of decision trees during training and outputs the mode, where the class that appears most frequently across the trees is selected as the overall prediction. By combining predictions from multiple trees, Random Forest enhances accuracy and generalization while mitigating overfitting. For this reason, we think RF might be a good next step. Let's see if we see improvements.\n",
    "\n",
    "We initialize a RF model, set n_estimators to the specified `num_trees`, fit the model to our training data, make predictions and get the WAP score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851dbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the desired number of trees\n",
    "num_trees = 50\n",
    "\n",
    "# Create an instance of RandomForestClassifier with the specified number of trees\n",
    "rf_model = RandomForestClassifier(n_estimators=num_trees, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train_array, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acfb78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "rf_y_pred = rf_model.predict(X_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "rf_report = classification_report(y_test, rf_y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", rf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique classes from y_test\n",
    "unique_classes = np.unique(y_test)\n",
    "\n",
    "# Generate confusion matrix\n",
    "rf_cm = confusion_matrix(y_test, rf_y_pred)\n",
    "rf_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d32ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=unique_classes, yticklabels=unique_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e5b9a",
   "metadata": {},
   "source": [
    "We see that the RF model demonstrates solid performance in identifying headlines with \"Low Bias,\" (again to be expected) achieving a high precision of 0.68 and high recall of 0.98, suggesting its effectiveness in correctly classifying low bias headlines. Again, the model struggles with headlines labeled as \"High Bias\" and \"No Bias,\" indicating a higher rate of false positives in these categories.\n",
    "\n",
    "**RF Weighted Average F1 score:** 57%, indicating a slightly poorer performance than our LogReg model by about 3%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7dc579",
   "metadata": {},
   "source": [
    "# Support Vector Machine<a id='Support Vector Machine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51712694",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) classify data points by finding the hyperplane that maximally separates different classes in the feature space, aiming to achieve the best margin between them. SVM can be a powerful next step; let's see if this is the case. We initialize, fit/train, make predictions, and outut scores and confusion matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca2250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of Support Vector Machine (SVM) classifier\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "# Training the model on the training data\n",
    "svm_model.fit(X_train_array, y_train)\n",
    "\n",
    "# Making predictions on the test data\n",
    "svm_y_pred = svm_model.predict(X_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating classification report\n",
    "svm_report = classification_report(y_test, svm_y_pred)\n",
    "\n",
    "# Printing the classification report\n",
    "print(\"Classification Report:\\n\", svm_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecdbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique classes from y_test\n",
    "unique_classes = np.unique(y_test)\n",
    "\n",
    "# Generate confusion matrix\n",
    "svm_cm = confusion_matrix(y_test, svm_y_pred)\n",
    "svm_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437092d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(svm_cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=unique_classes, yticklabels=unique_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Support Vector Machine Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8686a",
   "metadata": {},
   "source": [
    "Our SVM results show that for the \"High Bias\" category, the SVM model predicted 269 headlines correctly, 1218 headlines incorrectly as \"Low Bias,\" and none correctly as \"No Bias.\"\n",
    "For the \"Low Bias\" category, the model predicted 5159 headlines correctly, 85 headlines incorrectly as \"High Bias,\" and none correctly as \"No Bias.\"\n",
    "For the \"No Bias\" category, the model predicted none correctly for any category.\n",
    "\n",
    "**SVM Weighted Average F1 score:** 60%, the same as our LogReg model. Not terrible! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3473b0",
   "metadata": {},
   "source": [
    "# Complex Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a1d7f",
   "metadata": {},
   "source": [
    "It's time to try some more complex models to see if we can get better balance across categories and higher WAP scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d13cc6",
   "metadata": {},
   "source": [
    "## Neural Networks<a id='Neural Networks'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ee0d6",
   "metadata": {},
   "source": [
    "Neural networks are computational models inspired by the structure and function of the human brain, composed of interconnected nodes organized in layers. They excel in learning complex patterns and relationships from data, making them powerful for classification tasks like ours.\n",
    "\n",
    "We begin by importing all necessary libraries, modules and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4209f",
   "metadata": {},
   "source": [
    "These preprocessing steps are necessary when preparing categorical target variables for our neural networks: \n",
    "\n",
    "- Create and instance of `LabelEncoder:`, to convert class labels (strings or integers) into numerical format.\n",
    "- Store encoded versions of target variables as `y_train_encoded` and `y_test_encoded` The `fit_transform` method converts categorical labels into t/f values.\n",
    "\n",
    "- One-hot encode the above variables `to_categorical:`, which converts the numerical labels into binary vectors, where each element represents the presence or absence of a class.\n",
    "\n",
    "- Set `class_labels:` to better interpret model outputs and decoding predictions back into their original class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b587f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encoding y_test and y_train\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# One-hot encoding y_train_encoded and y_test_encoded\n",
    "y_train_one_hot = to_categorical(y_train_encoded)\n",
    "y_test_one_hot = to_categorical(y_test_encoded)\n",
    "\n",
    "# Setting class labels for later model output interpretation\n",
    "class_labels = np.unique(y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7555213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing class weights for use in neural network models\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=y_train_encoded)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ef2ad",
   "metadata": {},
   "source": [
    "We will now run a series of neural networks: a simple nn, nn with dropout rate, nn with L2 regularization, nn with batch normalization, and a multilayer perceptron with batch normalization. We will then evaluate which model performs the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca713e23",
   "metadata": {},
   "source": [
    "### Simple Neural Network (SNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46951d",
   "metadata": {},
   "source": [
    "First, a simple neural network architecture with 2 dense layers. Further explanation of the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the neural network model with a baseline architecture of 2 layers\n",
    "\n",
    "# Initialing a sequential model, which is a linear stack of layers\n",
    "nn_model = Sequential()\n",
    "# Adding first hidden layer with 64 neurons, input dimension determined by the number of features in the \n",
    "# training data (X_train_array), and ReLU activation function, which enables positive input \n",
    "#and filters negatives\n",
    "nn_model.add(Dense(64, input_dim=X_train_array.shape[1], activation='relu'))\n",
    "# Adding second hidden layer with 32 neurons and ReLU activation function\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "# Adding the output layer with the number of neurons equal to the number of unique classes in the target variable. \n",
    "# Using softmax activation for multi-class classification, which converts model output into probabilities\n",
    "nn_model.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compiles the model, specifying the Adam optimizer (an adaptive optimization algorithm for training), \n",
    "#categorical crossentropy as the loss function\n",
    "nn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model, running for 10 epochs with a batch size of 32, setting validation split of 20% and \n",
    "#specified previously calculated class weights \n",
    "nn_model.fit(X_train_array, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f3f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test data\n",
    "nn_model_y_pred_one_hot = nn_model.predict(X_test_array)\n",
    "\n",
    "# Converting the predicted probabilities to class labels\n",
    "nn_model_y_pred_classes = np.argmax(nn_model_y_pred_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39781f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing classification report for baseline neural network\n",
    "print(classification_report(y_test_encoded, nn_model_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78b4da",
   "metadata": {},
   "source": [
    "**Simple Neural Network Weighted Average F1 score:** 47%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31248e92",
   "metadata": {},
   "source": [
    "### Neural Network with Dropout Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6573c2db",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique that randomly sets a fraction (0.5) of input units to zero during training. This helps prevent overfitting by reducing reliance on specific neurons.\n",
    "\n",
    "We'll add dropout rates to our original architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef56684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the neural network model\n",
    "nn_model_2 = Sequential()\n",
    "nn_model_2.add(Dense(64, input_dim=X_train_array.shape[1], activation='relu'))\n",
    "nn_model_2.add(Dropout(0.5))  # Adding dropout rate of 0.5\n",
    "nn_model_2.add(Dense(32, activation='relu'))\n",
    "nn_model_2.add(Dropout(0.5))  # Adding dropout rate of 0.5\n",
    "nn_model_2.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "nn_model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "nn_model_2.fit(X_train_array, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce10d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "nn_model_2_y_pred_one_hot = nn_model_2.predict(X_test_array)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "nn_model_2_y_pred_classes = np.argmax(nn_model_2_y_pred_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_test_encoded, nn_model_2_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0009dbde",
   "metadata": {},
   "source": [
    "**NN with Dropout Rate Weighted Average F1 Score:** 51%\n",
    "\n",
    "This is an increase in 4% from our simple neural network's performance. Let's try L2 Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06216a9",
   "metadata": {},
   "source": [
    "### Neural Network with L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee07b1f5",
   "metadata": {},
   "source": [
    "L2 regularization, also known as weight decay, is a regularization technique in machine learning that adds a penalty term to the loss function, proportional to the square of the magnitude of the model's weights. It helps prevent overfitting by discouraging overly complex models with large weight values. \n",
    "\n",
    "Since we have class weights defined in fitting, we'll see if adding L2 Regularization helps us or hurts us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simple neural network model\n",
    "nn_model_3 = Sequential()\n",
    "nn_model_3.add(Dense(64, input_dim=X_train_array.shape[1], activation='relu', kernel_regularizer=l2(0.01)))\n",
    "nn_model_3.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "nn_model_3.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "nn_model_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "nn_model_3.fit(X_train_array, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "nn_model_3_y_pred_one_hot = nn_model_3.predict(X_test_array)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "nn_model_3_y_pred_classes = np.argmax(nn_model_3_y_pred_one_hot, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_encoded, nn_model_3_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d196cf",
   "metadata": {},
   "source": [
    "L2 definitely hurt us. We couldn't imagine a model performing worse than this. Regularization strength could be  high here, leading to excessive penalization of weights and hindering the model's ability to capture complex patterns in the data. \n",
    "\n",
    "**NN with L2 Regularization Weighted Average F1 Score:** 3%\n",
    "\n",
    "Perhaps more appropriate model architecture would help optimize this model, but we will move on to try Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf8b0d",
   "metadata": {},
   "source": [
    "### Neural Network with Batch Normalization\n",
    "Neural networks with Batch Normalization include an additional layer that normalizes the input at each training iteration, reducing internal covariate shift and accelerating training by maintaining stable activations. This helps improve convergence, generalization, and mitigates sensitivity to weight initialization.\n",
    "\n",
    "Let's see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368eba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "nn_model_4 = Sequential()\n",
    "nn_model_4.add(Dense(64, input_dim=X_train_array.shape[1], activation='relu'))\n",
    "nn_model_4.add(BatchNormalization())\n",
    "nn_model_4.add(Dense(32, activation='relu'))\n",
    "nn_model_4.add(BatchNormalization())\n",
    "nn_model_4.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "nn_model_4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "nn_model_4.fit(X_train_array, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205eccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "nn_model_4_y_pred_one_hot = nn_model_4.predict(X_test_array)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "nn_model_4_y_pred_classes = np.argmax(nn_model_4_y_pred_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc58b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_test_encoded, nn_model_4_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407afa0",
   "metadata": {},
   "source": [
    "We've achieved similar performance as our simple neural network with a dropout rate of 0.5, which is good, as we are back to baseline!\n",
    "\n",
    "**NN with Batch Normalization Weighted Average F1 score:** 51%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a08cb",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron (MLP) with Batch Normalization\n",
    "A multilayer perceptron (MLP) is a type of artificial neural network characterized by multiple layers of nodes (neurons) organized into an input layer, one or more hidden layers, and an output layer. It utilizes nonlinear activation functions to enable the model to learn complex relationships in the data, making it a versatile architecture for various machine learning tasks like this one. Let's see how it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "nn_model_5 = Sequential()\n",
    "nn_model_5.add(Dense(128, input_dim=X_train_array.shape[1], activation='relu'))\n",
    "nn_model_5.add(BatchNormalization())\n",
    "nn_model_5.add(Dense(64, input_dim=X_train_array.shape[1], activation='relu'))\n",
    "nn_model_5.add(BatchNormalization())\n",
    "nn_model_5.add(Dense(32, activation='relu'))\n",
    "nn_model_5.add(BatchNormalization())\n",
    "nn_model_5.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "nn_model_5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "nn_model_5.fit(X_train_array, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c5cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "nn_model_5_y_pred_one_hot = nn_model_5.predict(X_test_array)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "nn_model_5_y_pred_classes = np.argmax(nn_model_5_y_pred_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37133f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_test_encoded, nn_model_5_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162eac87",
   "metadata": {},
   "source": [
    "Looks like our F1 score has continued to remain stagnant. Despite our best efforts, we have yet to surpass 51%.\n",
    "\n",
    "**MLP with Batch Normalization Weighted Average F1 score:** 51%.\n",
    "\n",
    "It seems that all our neural networks models, aside from L2 Regularization, all with weighted average F1 scores of 51%, have at least improved upon the baseline  neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6969a8",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)<a id='LSTM'></a>\n",
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network architecture designed to address the vanishing gradient problem, allowing for the effective modeling of long-term dependencies in sequential data by maintaining a memory cell with input, forget, and output gates. LSTMs are widely used in natural language processing and classification tasks. Let's see how it does.\n",
    "\n",
    "First, we import LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1df600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d781b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model with LSTM with Batch Normalization and Dropout Rate\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(X_train_array.shape[1], 1), activation='relu'))\n",
    "lstm_model.add(BatchNormalization())\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(32, activation='relu'))\n",
    "lstm_model.add(BatchNormalization())\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights and a faster learning rate\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data for LSTM (assuming X_train_array has shape (samples, features))\n",
    "X_train_reshaped = X_train_array.reshape((X_train_array.shape[0], X_train_array.shape[1], 1))\n",
    "\n",
    "# Train the RNN model with class weights\n",
    "lstm_model.fit(X_train_reshaped, y_train_one_hot, epochs=5, batch_size=250, validation_split=0.2, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262010c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "lstm_model_y_pred_one_hot = lstm_model.predict(X_test_array)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "lstm_model_y_pred_classes = np.argmax(lstm_model_y_pred_one_hot, axis=1)\n",
    "\n",
    "# Convert true labels to class labels\n",
    "lstm_model_y_test_classes = np.argmax(y_test_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb477b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(lstm_model_y_test_classes, lstm_model_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2e7e2",
   "metadata": {},
   "source": [
    "We see a poor LSTM performance (Weighted Average F1 score of3%) similar to the NN with L2 regularization. Perhaps a dropout rate of 0.5 is too high, causing the network to lose too much information during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c110657",
   "metadata": {},
   "source": [
    "# Findings<a id='Findings'></a>\n",
    "It seems that all our neural networks models, aside from L2 Regularization, all with weighted average F1 scores of 51%, have at least improved upon the baseline neural network.\n",
    "\n",
    "**Best performing WAF1 scores:**\n",
    "- NN with Dropout: 51%\n",
    "- NN with Batch Normalization: 51%\n",
    "- MLP with Batch Normalization: 51%\n",
    "\n",
    "**Moderate WAF1 score:**\n",
    "- Simple NN: 47%\n",
    "\n",
    "**Poor WAF1 scores:**\n",
    "- NN with L2 Reg: 3%\n",
    "- LSTM with Batch Normalization and Dropout: 3%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da71f7",
   "metadata": {},
   "source": [
    "# Recommendations<a id='Recommendations'></a>\n",
    "\n",
    "Since best performing models involved adding batch normalization and dropout...\n",
    "- **Continue to test neural networks with batch normalization and different dropout rates**, employing activation functions like LeakyReLU, and experimenting with different batch sizes, number of epochs, and learning rates.\n",
    "- **Build on Multilayer Perceptron by adding layers.**\n",
    "- **Plot loss functions for best performing models** to better understand the dynamics of the training process and make informed decisions about model architecture\n",
    "\n",
    "### FairFrame: A Bias Mitigation Tool for Writers and Editors\n",
    "\n",
    "PBS needs to back up its reputation as an objective American news source and defend itself against actors threatening to defund it on accusations of bias. In order to achieve this, we propose FairFrame - a simple app or pop-up browser extension  for writers and editors at PBS. This tool uses optimized neural networks identifying bias categories in headlines throughout the editorial process.\n",
    "\n",
    "- **Headline Input:** Users can feed headlines into the tool for analysis.\n",
    "- **Bias Categorization:** The tool employs a pretrained model to categorize the bias of the content into different levels (e.g., Neutral, Low Bias, High Bias).\n",
    "- **Guided Editing Notes:** For content flagged with bias, the tool's various pre-loaded functions provide users with actionable and specific editing suggestions. For instance: \"Consider neutralizing language in this sentence.\" OR \"Use active voice for greater clarity and objectivity.\" OR \"Adjust the tone to ensure a balanced representation.\"\n",
    "- **User-Friendly Interface:** The tool features an intuitive and user-friendly interface, ensuring seamless integration into the editorial workflow.\n",
    "\n",
    "**The Bottom Line:** FairFrame has the potential to enhance objectivity, empower writers and editors, hold all writers and editors to consistent standards, and promote efficient editing.\n",
    "\n",
    "**Implementation**:\n",
    "- Collaborate with machine learning experts to fine-tune the bias categorization model based on PBS's editorial guidelines.\n",
    "- Conduct thorough training sessions for writers and editors to ensure effective utilization of the tool.\n",
    "    \n",
    "**Additional Recommendations:**\n",
    "- Publish each article's headline bias score in plain view for readers to see\n",
    "- Advertise and demonstrate the use and efficacy of FairFrame in public funding defense efforts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37999c6",
   "metadata": {},
   "source": [
    "# Limitations and Further Inquiry<a id='Limitations and Further Inquiry'></a>\n",
    "\n",
    "The scope and time constraints of the project limit our ability to explore other possible optimizations, activations, and other hyperparameters. To address this...\n",
    "- **Explore more Recurrent Neural Networks like Bidirectional LSTM and Gated Recurrent Unit (GRU).**\n",
    "\n",
    "The scope and time constraints of the project limit our ability to explore the dataframe's fullest modeling potential. To address this...\n",
    "- **Invest in including some or all features engineered throughout our Feature Engineering (and more EDA) section.** We tried running comprehensive models using one or more of the engineered features, both one-hot-encoded categorical features and numerical features. It caused our models to run very slowly and in the interest of time and the scope of our project, we decided to leave it up to PBS to prepare and include them in future model iterations.\n",
    "- **Merge other DataFrames from the broader data.world dataset (linked in dataset overview) to headlines.csv** and see what other comprehensive set of features can be gleaned.\n",
    "\n",
    "Our data is highly imbalanced toward the Low Bias category. To address this...\n",
    "- **Utilize the SMOTE library or a variant of it to address class imbalance head on.** These librares generate synthetic examples of the minority class to create a more balanced distribution and could lead to better optimized models.\n",
    "- **Calculate class weights to customize to each individual model.**\n",
    "\n",
    "- **Invest in collecting or utilizing existing PBS-specific headline data and other data from more reputable sources** to increase the relevance and efficacy of FairFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f161f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
