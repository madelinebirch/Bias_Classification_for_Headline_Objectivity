{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a32d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pd.read_csv('Data/headlines.csv')\n",
    "\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns url, unnamed, and index\n",
    "headlines = headlines.drop(columns=['url', 'Unnamed: 0', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a53220",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41497d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting value counts for bias feature\n",
    "headlines['bias'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting bias distribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot histogram of 'bias'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines['bias'], bins=20, kde=True, color='blue')\n",
    "\n",
    "# Style the plot\n",
    "plt.title('Distribution of Bias Scores', fontsize=16)\n",
    "plt.xlabel('Bias Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating feature 'sentiment_polarity'\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create a sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply the sentiment analyzer to each headline and store the compound score\n",
    "headlines['sentiment_polarity'] = headlines['headline_no_site'].apply(lambda x: sid.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34355de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['bias'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b643b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['sentiment_polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b3bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print min and max values for sentiment polarity\n",
    "print(headlines['sentiment_polarity'].min())\n",
    "print(headlines['sentiment_polarity'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbbdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dist of sentiment polarity\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot histogram of 'sentiment_polarity'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines['sentiment_polarity'], bins=20, kde=True, color='green')\n",
    "\n",
    "# Style the plot\n",
    "plt.title('Distribution of Sentiment Polarity Scores', fontsize=16)\n",
    "plt.xlabel('Sentiment Polarity Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Date Features\n",
    "headlines['Day_of_Week'] = pd.to_datetime(headlines['time']).dt.day_name()\n",
    "headlines['Month'] = pd.to_datetime(headlines['time']).dt.month\n",
    "# Time feature\n",
    "headlines['Hour_of_Day'] = pd.to_datetime(headlines['time']).dt.hour\n",
    "# Assuming 'time' is the feature containing datetime information\n",
    "headlines['time'] = pd.to_datetime(headlines['time'], errors='coerce')\n",
    "# Extract the year and create a new feature 'Publication_Year'\n",
    "headlines['Publication_Year'] = headlines['time'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping time column\n",
    "headlines = headlines.drop(columns=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e490257",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c053ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word count feature\n",
    "headlines['Word_Count'] = headlines['headline_no_site'].apply(lambda x: len(x.split()))\n",
    "# Creating text length feature\n",
    "headlines['Text_Length'] = headlines['headline_no_site'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee651e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261aa9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7efca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['site'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'headlines' is the name of your dataframe\n",
    "min_headlines_threshold = 5000\n",
    "top_sites = headlines['site'].value_counts()\n",
    "top_sites = top_sites[top_sites >= min_headlines_threshold].index\n",
    "\n",
    "# Create a new dataframe with only the sites with at least 5000 headlines\n",
    "headlines_filtered = headlines[headlines['site'].isin(top_sites)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'headlines_filtered' is the name of your dataframe\n",
    "top_10_sites = headlines_filtered['site'].value_counts().nlargest(10)\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(top_10_sites, labels=top_10_sites.index, autopct='%1.1f%%', colors=sns.color_palette('viridis'), startangle=90)\n",
    "plt.title('Top 10 News Sources Distribution')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9688e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'headlines_filtered' is the name of your dataframe\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='country', data=headlines_filtered, palette='viridis')\n",
    "plt.title('Distribution of Countries')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of Headlines')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77904be",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48841851",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered['bias'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4335f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Assuming 'headlines_filtered' is your DataFrame\n",
    "conditions = [\n",
    "    headlines_filtered['bias'].between(0.000000, 0.2, inclusive='both'),\n",
    "    headlines_filtered['bias'].between(0.2, 0.4, inclusive='both'),\n",
    "    headlines_filtered['bias'].between(0.4, 0.6, inclusive='both'),\n",
    "    headlines_filtered['bias'].between(0.6, 0.8, inclusive='both'),\n",
    "    headlines_filtered['bias'].between(0.8, 1.0, inclusive='both'),\n",
    "]\n",
    "\n",
    "labels = ['No Bias', 'Low Bias', 'Moderate Bias', 'Medium Bias', 'High Bias']\n",
    "\n",
    "headlines_filtered['bias_category'] = np.select(conditions, labels, default=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f89f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered['bias_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8956b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping original bias column\n",
    "headlines_filtered = headlines_filtered.drop(columns=['bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e5cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcabae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines_filtered['Word_Count'], bins=30, color='skyblue', kde=False)\n",
    "plt.title('Distribution of Word Count in Headlines')\n",
    "plt.xlabel('Word_Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e022abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines_filtered['Text_Length'], bins=30, color='skyblue', kde=False)\n",
    "plt.title('Distribution of Text Length in Headlines')\n",
    "plt.xlabel('Word_Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc87815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting the distribution of bias_category by Publication_Year\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x=\"Publication_Year\", hue=\"bias_category\", data=headlines_filtered)\n",
    "plt.title('Distribution of Bias Category by Publication_Year')\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f57f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot a swarm plot for Sentiment_Polarity vs. bias with a gradient color scheme\n",
    "plt.figure(figsize=(12, 6))\n",
    "scatter = sns.scatterplot(x='sentiment_polarity', y='bias_category', data=headlines_filtered, hue='bias_category', palette='viridis', size=3)\n",
    "\n",
    "# Style the plot\n",
    "plt.title('Distribution of Sentiment Polarity for Different Bias Categories', fontsize=16)\n",
    "plt.xlabel('Sentiment Polarity', fontsize=12)\n",
    "plt.ylabel('Bias Category', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Create a ScalarMappable for the colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis')\n",
    "sm.set_array([])  # Set an empty array\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fcc78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the categorical columns to one-hot encode\n",
    "categorical_columns = ['site', 'country', 'Day_of_Week', 'Month', 'Hour_of_Day', 'Publication_Year']\n",
    "\n",
    "# Create one-hot encoded columns with 1s and 0s\n",
    "one_hot_encoded = pd.get_dummies(headlines_filtered[categorical_columns], drop_first=True, dtype=int)\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "headlines_filtered_encoded = pd.concat([headlines_filtered, one_hot_encoded], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "headlines_filtered_encoded.drop(categorical_columns, axis=1, inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "headlines_filtered_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1326b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'headlines_no_site' column to 'headlines'\n",
    "headlines_filtered_encoded.rename(columns={'headline_no_site': 'headlines'}, inplace=True)\n",
    "\n",
    "headlines_filtered_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3053e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Tokenize the headline text\n",
    "headlines_filtered_encoded['tokenized_text'] = headlines_filtered_encoded['headlines'].apply(word_tokenize)\n",
    "\n",
    "# Remove non-alphabetic characters, handle empty strings, and extra spaces\n",
    "headlines_filtered_encoded['cleaned_text'] = headlines_filtered_encoded['tokenized_text'].apply(lambda tokens: [re.sub(r'[^a-zA-Z0-9]', '', token).strip() for token in tokens if re.sub(r'[^a-zA-Z0-9]', '', token).strip()])\n",
    "\n",
    "# Convert to lowercase\n",
    "headlines_filtered_encoded['cleaned_text'] = headlines_filtered_encoded['cleaned_text'].apply(lambda tokens: [token.lower() for token in tokens])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "headlines_filtered_encoded['lemmatized_text'] = headlines_filtered_encoded['cleaned_text'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad9bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_df = headlines_filtered_encoded.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c3ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'headlines' column from lemmatized_df\n",
    "lemmatized_df.drop('headlines', axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows of lemmatized_df after dropping the column\n",
    "lemmatized_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e3c98",
   "metadata": {},
   "source": [
    "remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from the lemmatized_text column\n",
    "lemmatized_df['lemmatized_text_no_stopwords'] = lemmatized_df['lemmatized_text'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1168b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_df['lemmatized_text_no_stopwords'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe943c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_vectorize = lemmatized_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['tokenized_text', 'cleaned_text', 'lemmatized_text']\n",
    "\n",
    "# Drop the specified columns\n",
    "df_to_vectorize.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70fc267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_vectorize.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_vectorize['lemmatized_text_no_stopwords'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe281ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "import gensim\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa26b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model and assign it to 'model'\n",
    "model = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "#We can also print a list to show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec95956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text):\n",
    "    \"\"\"\n",
    "    Embed a text by averaging the word vectors of the tokenized text. \n",
    "    Put embeddings into 300 dimensions.\n",
    "    Out-of-vocabulary words are replaced by the zero-vector.\n",
    "    Stop words are removed.\n",
    "    -----\n",
    "    \n",
    "    Input: text (string)\n",
    "    Output: embedding vector (np.array)\n",
    "    \"\"\"\n",
    "    # Assuming text is already tokenized and lemmatized\n",
    "    tokenized = text\n",
    "    \n",
    "    # We want our embeddings in 300 dimensions\n",
    "    word_embeddings = [np.zeros(300)]\n",
    "    for word in tokenized:\n",
    "        # if the word is in the model then embed\n",
    "        if word in model:\n",
    "            vector = model[word]\n",
    "        # add zeros for out-of-vocab words that are not in the pretrained embedding model \n",
    "        else:\n",
    "            vector = np.zeros(300)\n",
    "            \n",
    "        word_embeddings.append(vector)\n",
    "    \n",
    "    # Average the word vectors with .mean()\n",
    "    text_embedding = np.stack(word_embeddings).mean(axis=0)\n",
    "    \n",
    "    return text_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use lambda to apply the function over the columns and assign the results to new columns\n",
    "df_to_vectorize['title_vectors'] = df_to_vectorize['lemmatized_text_no_stopwords'].apply(lambda x: text2vec(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb0741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_vectorize['title_vectors'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_vectorize.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd87298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_vectorize.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_to_vectorize.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef513c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'lemmatized_text_no_stopwords' column\n",
    "final_df = final_df.drop('lemmatized_text_no_stopwords', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457190ba",
   "metadata": {},
   "source": [
    "scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801a0614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
