{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca235d04",
   "metadata": {},
   "source": [
    "## Gender Bias Headline Classification Modeling as a Foundation for News Objectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4fd056",
   "metadata": {},
   "source": [
    "Madeline F. Birch | November 2023 | Flatiron School Data Science Program | Final Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7ca2b",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. [Project Overview](#Project_Overview)\n",
    "2. [The Dataset](#The_Dataset)\n",
    "3. [Exploratory Data Analysis](#EDA)\n",
    "4. [Feature Engineering (and more EDA)](#Feature_Engineering)\n",
    "5. [Preprocessing](#Preprocessing)\n",
    "6. [Train Test Split](#Train_Test_Split)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda119b7",
   "metadata": {},
   "source": [
    "# Project Overview<a id='Project_Overview'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d1d81",
   "metadata": {},
   "source": [
    "In an era where information shapes perspectives, the media plays a pivotal role in influencing societal narratives. Understanding the subtle nuances and potential biases embedded in headlines is crucial, and this project aims to shed light on the degree of bias present in headlines. Focusing on data classification through machine learning, the project seeks to predict headlines into three classes: No Bias, Low Bias, and High Bias. The objective is not to scrutinize sensationalism or analyze sentiment polarity but to leverage textual and numerical features to predict the bias level accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d9d5d",
   "metadata": {},
   "source": [
    "<img src=\"Images/pbs_logo.png\" alt=\"PBS Logo\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9e6caf",
   "metadata": {},
   "source": [
    "### Our Stakeholder: PBS News\n",
    "The significance of this project lies in its potential impact on journalism's objectivity, particularly for PBS News, a revered American news source known for its impartiality and lack of apparent agenda. Publicly funded by 15%, the entity continuously faces accusations of general bias and [threats of defunding from various actors](https://www.nytimes.com/2011/02/28/business/media/28cpb.html). By adopting insights gained from our efforts,  PBS News can silence these threats and secure its value as a trusted, objective news source.\n",
    "\n",
    "### Our Vision: Gender Bias in Headlines as a Framework\n",
    "We chose to focus on gender bias as a focus for this project because it is undeniably one of the most prevalent forms of bias in published news content. A 2021 Topic Modeling [study](https://www.frontiersin.org/articles/10.3389/frai.2021.664737/full) found women are unfortunately but not unexpectedly mentioned \"more frequently in topics related to lifestyle, healthcare, and crimes and sexual assault.\" Another 2021 Natural Language Processing [study](file:///Users/madelinebirch/Downloads/journal.pone.0245533.pdf) concluded, \"although\n",
    "we see a certain tokenism in having female voices present in the news, their voices are drowned\n",
    "out by the overwhelming number of times that we hear from men, often from just a handful of\n",
    "men.\" There's no denying that gender bias is present in news articles themselves, but what about the content of headlines? Couldn't headlines *about* women be biased, too?\n",
    "\n",
    "At the heart of this initiative is the recognition of headlines as powerful agents that shape our perceptions. These succinct phrases captivate our attention and mold our subconscious understanding of entire articles. The urgency to prove and maintain neutrality, especially on a platform as eminent as PBS, underscores the relevance of our undertaking.\n",
    "\n",
    "While our primary focus is on demonstrating which ML algorithms are most adept at detecting gender bias in headlines, this project could serve a framework for ongoing assessments of headlines across diverse bias types, including political, racial, LGBTQ+, socioeconomic class and beyond. The broader vision is to contribute to a media landscape characterized by transparency, objectivity, and accountability, fostering a public discourse grounded in fair and unbiased reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c73119",
   "metadata": {},
   "source": [
    "# The Dataset<a id='The_Dataset'></a>\n",
    "\n",
    "The dataset utilized in this project originates from a comprehensive collection of data scraped for [\"When Women Make Headlines,\"](https://pudding.cool/2022/02/women-in-headlines/) a visual essay published by [*The Pudding*](https://pudding.cool/) in June 2022. Released by [Amber Thomas](https://data.world/amberthomas) on [data.world](https://data.world/the-pudding/women-in-headlines), this dataset is one of few open source datasets we could find that investigates gender bias specifically in headlines. It encompasses a diverse array of headlines about women, each annotated with corresponding bias scores. Bias scores were calculated following the methodology outlined in [\"Proposed Taxonomy for Gender Bias in Text; A Filtering Methodology for the Gender Generalization Subtype.\"](https://aclanthology.org/W19-3802.pdf)\n",
    "\n",
    "The dataset's origin in a visual essay adds an element of real-world applicability, grounding the project in the practical considerations of media consumption and perception. Its richness of lies in its amalgamation of both textual numerical features associated with each headline. The text data provides the linguistic context of the headlines, while numerical features offer additional dimensions for analysis. This holistic approach enables the development of a machine learning model that can discern patterns beyond linguistic constructs, contributing to a nuanced understanding of what contributes to bias. \n",
    "\n",
    "### Our Working File\n",
    "The dataset contains a multitude of `.csv` files; for the sake of simplicity, we will be working exclusively with `headlines.csv` and engineering additional numerical features to bolster our models.\n",
    "\n",
    "### Our Target\n",
    "Our target variable will be `bias`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2cb355",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis<a id='EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bdf7eb",
   "metadata": {},
   "source": [
    "Let's begin our EDA by:\n",
    "1) importing all necessary libraries, packages and modules necessary for exploring and visualizing our data,\n",
    "2) loading and inspecting our `headlines.csv` file as a DataFrame,\n",
    "3) seeing if any irrelevant columns need dropping,\n",
    "4) getting the shape of our DataFrame,\n",
    "5) getting primary statistics and, \n",
    "6) inspecting the distribution of our target variable `bias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ae384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 'headlines.csv' into a Pandas DataFrame\n",
    "headlines = pd.read_csv('Data/headlines.csv')\n",
    "\n",
    "# Showing first 5 rows of the DataFrame\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a192e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns 'url', 'Unnamed: 0', and 'index'\n",
    "headlines = headlines.drop(columns=['url', 'Unnamed: 0', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5334c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the shape of our DataFrame\n",
    "headlines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting primary statistics\n",
    "headlines.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting value counts for bias feature\n",
    "headlines['bias'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c44f47",
   "metadata": {},
   "source": [
    "We see above that our target, though numerical in nature, is ultimately categorical, as there are only 6 distinct bias scores. We'll deal with that further along in data processing. We also see that it is a highly imbalanced feature. Let's plot to confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting bias distribution\n",
    "\n",
    "# Setting Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting histogram of 'bias'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines['bias'], bins=20, kde=True, color='blue')\n",
    "\n",
    "# Styling the plot\n",
    "plt.title('Distribution of Bias Scores', fontsize=16)\n",
    "plt.xlabel('Bias Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Showing the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e9085",
   "metadata": {},
   "source": [
    "# Feature Engineering (and more EDA) <a id='Feature_Engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00659a",
   "metadata": {},
   "source": [
    "Since our DataFrame contains mostly text features, let's introduce a numerical feature indicating the sentiment polarity of each headline. To accomplish this, we employ a sentiment analyzer using the SentimentIntensityAnalyzer from the Natural Language Toolkit (nltk) library. The goal is to capture the overall sentiment expressed in each headline.\n",
    "\n",
    "- We create a sentiment analyzer object, `sid`, using the SentimentIntensityAnalyzer.\n",
    "    - The analyzer is then applied to each headline in the 'headline_no_site' column.\n",
    "    - The resulting compound score, indicative of the overall sentiment, is stored in the new 'sentiment_polarity' feature.\n",
    "    - We then get value counts, min/max values, and plot the distribution of the feature\n",
    "\n",
    "*Please note that the sentiment analysis process may take some time to run, as we have a very large dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea265b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating feature 'sentiment_polarity'\n",
    "\n",
    "# Creating a sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Applying the sentiment analyzer to each headline and storing the compound score - this takes a while to run\n",
    "headlines['sentiment_polarity'] = headlines['headline_no_site'].apply(lambda x: sid.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf3485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting distribution of sentiment_polarity\n",
    "headlines['sentiment_polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191638ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing min and max values for sentiment polarity\n",
    "print(headlines['sentiment_polarity'].min())\n",
    "print(headlines['sentiment_polarity'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c36dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dist of sentiment polarity\n",
    "\n",
    "# Setting Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting histogram of 'sentiment_polarity'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines['sentiment_polarity'], bins=20, kde=True, color='green')\n",
    "\n",
    "# Styling the plot\n",
    "plt.title('Distribution of Sentiment Polarity Scores', fontsize=16)\n",
    "plt.xlabel('Sentiment Polarity Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Showing the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945d511",
   "metadata": {},
   "source": [
    "Another highly imbalanced feature. Hopefully, since we are using classification modeling, we won't need to worry about this too much. Classification models often produce probability scores or decision values that can be thresholded to make predictions. These scores are not affected by feature imbalances since they represent the model's confidence in predicting a certain class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9775891",
   "metadata": {},
   "source": [
    "Let's move on with some more feature engineering, this time extractin temporal data:\n",
    "\n",
    "- **Day of the Week and Month:**\n",
    "  - We create two new features, 'Day_of_Week' and 'Month,' by extracting information from the 'time' column. 'Day_of_Week' indicates the day on which the headline was published, while 'Month' represents the numerical month.\n",
    "\n",
    "- **Hour of the Day:**\n",
    "  - Another temporal feature, 'Hour_of_Day,' is engineered to capture the specific hour of publication. This fine-grained detail could reveal patterns related to the time of day.\n",
    "\n",
    "- **Publication Year:**\n",
    "  - The 'time' column is converted to a datetime format, and the year information is extracted to create a new feature, 'Publication_Year.' This allows us to analyze trends and biases over different years.\n",
    "\n",
    "The 'time' column is also converted to datetime format, ensuring accurate extraction of temporal features. Any errors during the conversion are coerced to handle potential issues with the datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering 'Day of the Week' and 'Month' Features\n",
    "headlines['Day_of_Week'] = pd.to_datetime(headlines['time']).dt.day_name()\n",
    "headlines['Month'] = pd.to_datetime(headlines['time']).dt.month\n",
    "\n",
    "# Engineering 'Hour of Dat' feature\n",
    "headlines['Hour_of_Day'] = pd.to_datetime(headlines['time']).dt.hour\n",
    "\n",
    "# Converting 'time' column to datetime format\n",
    "headlines['time'] = pd.to_datetime(headlines['time'], errors='coerce')\n",
    "\n",
    "# Extracting the year and creating a new 'Publication Year' feature\n",
    "headlines['Publication_Year'] = headlines['time'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a845d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping time column\n",
    "headlines = headlines.drop(columns=['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e35e4",
   "metadata": {},
   "source": [
    "Now, to get word count and text length values for each headline:\n",
    "\n",
    "- **Word Count:**\n",
    "  - We create a `Word_Count` feature by applying a lambda function to calculate the number of words in each headline. This feature provides insight into the lexical richness and complexity of the headlines.\n",
    "\n",
    "- **Text Length:**\n",
    "  - A `Text_Length` feature is engineered by computing the length of each headline using the `len` function. This feature captures the overall character count, offering an additional perspective on the headline's brevity or verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876fe0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word count feature\n",
    "headlines['Word_Count'] = headlines['headline_no_site'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Creating text length feature\n",
    "headlines['Text_Length'] = headlines['headline_no_site'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e79b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting new head\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e566d25",
   "metadata": {},
   "source": [
    "Moving on to `site` and `country`. Let's check value counts for distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting value counts for site\n",
    "headlines['site'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting value counts for country\n",
    "headlines['country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a6a7a",
   "metadata": {},
   "source": [
    "To manage the size of our dataset, we apply a threshold to consider only news sites with a substantial volume of headlines.\n",
    "\n",
    "- We set a threshold of at least 5000 headlines for inclusion.\n",
    "- We identify the top news sites meeting or exceeding this threshold using the `site` column.\n",
    "- A new DataFrame, `headlines_filtered,` is created to contain only the headlines from the selected news sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a threshold for news sites with at least 5000 headlines\n",
    "min_headlines_threshold = 5000\n",
    "top_sites = headlines['site'].value_counts()\n",
    "top_sites = top_sites[top_sites >= min_headlines_threshold].index\n",
    "\n",
    "# Creating a new dataframe with only the sites with at least 5000 headlines\n",
    "headlines_filtered = headlines[headlines['site'].isin(top_sites)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ad2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting shape of new DataFrame\n",
    "headlines_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb3bbb",
   "metadata": {},
   "source": [
    "We'll nowcreate a DataFrame `top_10_sites` and apply `.nlargest` function to get the top 10 sites with the most headlines. Then, we plot the distribution of top 10 news sources in a pie chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c89d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting top 10 sites with most headlines\n",
    "top_10_sites = headlines_filtered['site'].value_counts().nlargest(10)\n",
    "\n",
    "# Creating a pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(top_10_sites, labels=top_10_sites.index, autopct='%1.1f%%', colors=sns.color_palette('viridis'), startangle=90)\n",
    "plt.title('Top 10 News Sources Distribution')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59858514",
   "metadata": {},
   "source": [
    "Now, let's plot the distribution of countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting distribution of countries\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='country', data=headlines_filtered, palette='viridis')\n",
    "plt.title('Distribution of Countries')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of Headlines')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde1b0a",
   "metadata": {},
   "source": [
    "Let's continue our EDA of engineered features like `Word_Count` and `Text_Length` by plotting histograms of each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81dd860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines_filtered['Word_Count'], bins=30, color='skyblue', kde=False)\n",
    "plt.title('Distribution of Word Count in Headlines')\n",
    "plt.xlabel('Word_Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ea046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(headlines_filtered['Text_Length'], bins=30, color='skyblue', kde=False)\n",
    "plt.title('Distribution of Text Length in Headlines')\n",
    "plt.xlabel('Word_Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bfebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting distribution of bias in headlines_filtered\n",
    "headlines_filtered['bias'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53385dc",
   "metadata": {},
   "source": [
    "It looks like there are few instances of headlines with bias scores of 0.666667 and 0.833333. We will drop them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fed9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting values to drop\n",
    "values_to_drop = [0.666667, 0.833333]\n",
    "\n",
    "# Use boolean indexing to drop rows with specified values in 'bias' column\n",
    "headlines_filtered = headlines_filtered[~headlines_filtered['bias'].isin(values_to_drop)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf0231d",
   "metadata": {},
   "source": [
    "We will now introduce a new feature, 'bias_category,' by categorizing the 'bias' values into distinct levels. The categorization is based on predefined conditions:\n",
    "\n",
    "- **No Bias:**\n",
    "  - Values equal to 0.0 are categorized as having no bias.\n",
    "\n",
    "- **Low Bias:**\n",
    "  - Values between 0.1 and 0.2 (inclusive) are considered to indicate low bias.\n",
    "\n",
    "- **High Bias:**\n",
    "  - Values between 0.3 and 0.5 (inclusive) fall into the high bias category.\n",
    "\n",
    "The `np.select` function efficiently applies these conditions, allowing us to create a categorical feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3751d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining conditions\n",
    "conditions = [\n",
    "    headlines_filtered['bias'].between(0.000000, 0.000000, inclusive='both'),\n",
    "    headlines_filtered['bias'].between(0.1, 0.2, inclusive='both'),\n",
    "    headlines_filtered['bias'].between(0.3, 0.5, inclusive='both'),\n",
    "]\n",
    "\n",
    "# Setting category labels\n",
    "labels = ['No Bias', 'Low Bias', 'High Bias']\n",
    "\n",
    "# Applying conditions\n",
    "headlines_filtered['bias_category'] = np.select(conditions, labels, default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d9979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting unique bias category values\n",
    "headlines_filtered['bias_category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c042e",
   "metadata": {},
   "source": [
    "We see that in the process of categorizing bias, we have raised some None-type values. We will deal with this shortly. First let's inspect our new head and drop the original `bias` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74edf5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing new head\n",
    "headlines_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb02a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping original bias column\n",
    "headlines_filtered = headlines_filtered.drop(columns=['bias'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d99393",
   "metadata": {},
   "source": [
    "We continue our EDA by plotting some key feature interactions that might be interesting to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of bias_category by Publication_Year\n",
    "\n",
    "# Setting the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Setting fig size, plotting feature interaction, setting axes titles and labels\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x=\"Publication_Year\", hue=\"bias_category\", data=headlines_filtered)\n",
    "plt.title('Distribution of Bias Category by Publication_Year')\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafac5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a swarm plot for Sentiment_Polarity vs. bias with a gradient color scheme\n",
    "\n",
    "# Setting a Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Setting fig size, creating plot of feature interaction\n",
    "plt.figure(figsize=(12, 6))\n",
    "scatter = sns.scatterplot(x='sentiment_polarity', y='bias_category', data=headlines_filtered, hue='bias_category', palette='viridis', size=3)\n",
    "\n",
    "# Styling the plot\n",
    "plt.title('Distribution of Sentiment Polarity for Different Bias Categories', fontsize=16)\n",
    "plt.xlabel('Sentiment Polarity', fontsize=12)\n",
    "plt.ylabel('Bias Category', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Creating a ScalarMappable for the colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis')\n",
    "sm.set_array([])  # Set an empty array\n",
    "\n",
    "# Showing the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa28ad",
   "metadata": {},
   "source": [
    "# Pre-processing<a id='Preprocessing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493183ad",
   "metadata": {},
   "source": [
    "To enhance the effectiveness of our machine learning model, we employ one-hot encoding for our many categorical features. This process involves:\n",
    "\n",
    "- **Selecting Categorical Columns:**\n",
    "  - We identify the columns eligible for one-hot encoding, including 'site,' 'country,' 'Day_of_Week,' 'Month,' 'Hour_of_Day,' and 'Publication_Year.'\n",
    "\n",
    "- **Creating One-Hot Encoded Columns:**\n",
    "  - Using the `pd.get_dummies` function, we convert the selected categorical columns into binary-encoded columns with 1s and 0s, dropping the first category to avoid multicollinearity.\n",
    "\n",
    "- **Concatenating with the Original DataFrame:**\n",
    "  - The one-hot encoded columns are concatenated with the original DataFrame, 'headlines_filtered,' creating a more feature-rich dataset.\n",
    "\n",
    "- **Dropping Original Categorical Columns:**\n",
    "  - The original categorical columns are dropped from the DataFrame, leaving behind the one-hot encoded representations.\n",
    "\n",
    "This process enhances the model's ability to interpret and learn from categorical information, contributing to a more robust analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86972a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the categorical columns to one-hot encode\n",
    "categorical_columns = ['site', 'country', 'Day_of_Week', 'Month', 'Hour_of_Day', 'Publication_Year']\n",
    "\n",
    "# Creating one-hot encoded columns with 1s and 0s\n",
    "one_hot_encoded = pd.get_dummies(headlines_filtered[categorical_columns], drop_first=True, dtype=int)\n",
    "\n",
    "# Concatenating the one-hot encoded columns with the original DataFrame\n",
    "headlines_filtered_encoded = pd.concat([headlines_filtered, one_hot_encoded], axis=1)\n",
    "\n",
    "# Dropping the original categorical columns\n",
    "headlines_filtered_encoded.drop(categorical_columns, axis=1, inplace=True)\n",
    "\n",
    "# Displaying first row of resulting DataFrame\n",
    "headlines_filtered_encoded.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bee853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the 'headlines_no_site' column to 'headlines'\n",
    "headlines_filtered_encoded.rename(columns={'headline_no_site': 'headlines'}, inplace=True)\n",
    "\n",
    "headlines_filtered_encoded['headlines'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5843a",
   "metadata": {},
   "source": [
    "To preprocess our text feature of note, `headlines`, we will need to import the following packages and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9da9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52504a7b",
   "metadata": {},
   "source": [
    "We now perform essential text preprocessing steps to enhance the quality of our textual data for machine learning analysis. The process includes:\n",
    "\n",
    "- **Tokenization:**\n",
    "  - Headline text is tokenized using the `word_tokenize` function, breaking down sentences into individual words.\n",
    "\n",
    "- **Cleaning Text:**\n",
    "  - Non-alphabetic characters are removed, empty strings are handled, and extra spaces are addressed. This ensures a cleaner representation of the text.\n",
    "\n",
    "- **Converting to Lowercase:**\n",
    "  - All tokens are converted to lowercase, providing a standardized format for analysis.\n",
    "\n",
    "- **Lemmatization:**\n",
    "  - lemmatization is the process of reducing words in the headline text to their base or root form. \n",
    "  - Lemmatization is applied using the WordNet lemmatizer from the Natural Language Toolkit (nltk). This step reduces words to their base or root form, aiding in feature extraction and improving the model's ability to discern patterns.\n",
    "\n",
    "Please note that the lemmatization process may take a minute or two to run, considering its comprehensive nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, Clean, and Lemmatize Text\n",
    "\n",
    "# Tokenizing the headline text\n",
    "headlines_filtered_encoded['tokenized_text'] = headlines_filtered_encoded['headlines'].apply(word_tokenize)\n",
    "\n",
    "# Removing non-alphabetic characters, handle empty strings, and extra spaces\n",
    "headlines_filtered_encoded['cleaned_text'] = headlines_filtered_encoded['tokenized_text'].apply(lambda tokens: [re.sub(r'[^a-zA-Z0-9]', '', token).strip() for token in tokens if re.sub(r'[^a-zA-Z0-9]', '', token).strip()])\n",
    "\n",
    "# Converting to lowercase\n",
    "headlines_filtered_encoded['cleaned_text'] = headlines_filtered_encoded['cleaned_text'].apply(lambda tokens: [token.lower() for token in tokens])\n",
    "\n",
    "# Lemmatization - this takes a minute or two to run\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "headlines_filtered_encoded['lemmatized_text'] = headlines_filtered_encoded['cleaned_text'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing a copy of headlines_filtered_encoded as lemmatized_df\n",
    "lemmatized_df = headlines_filtered_encoded.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'headlines' column from lemmatized_df\n",
    "lemmatized_df.drop('headlines', axis=1, inplace=True)\n",
    "\n",
    "# Displaying the first row of lemmatized_df after dropping the column\n",
    "lemmatized_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ca9cb",
   "metadata": {},
   "source": [
    "We now take a crucial step in refining our textual data for analysis by removing common English stop words. The process involves:\n",
    "\n",
    "- **Acquiring Stop Words:**\n",
    "  - We obtain a set of English stop words using the `stopwords.words('english')` function from the Natural Language Toolkit (nltk).\n",
    "\n",
    "- **Filtering Stop Words:**\n",
    "  - Stop words are then removed from the 'lemmatized_text' column using a lambda function. This helps eliminate frequently occurring words that typically don't contribute significant meaning to the analysis.\n",
    "\n",
    "By eliminating stop words, we focus the terms with the most meaning.\n",
    "\n",
    "We will then drop `tokenized_text,` `cleaned_text,` and `lemmatized_text.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Creating no stopwords feature, removing stop words from the lemmatized_text column\n",
    "lemmatized_df['lemmatized_text_no_stopwords'] = lemmatized_df['lemmatized_text'].apply(lambda tokens: [token for token in tokens if token not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a984e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting head of lemmatized_text_no_stopwords feature\n",
    "lemmatized_df['lemmatized_text_no_stopwords'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45a1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing a copy of lemmatized_df as df_to_vectorize\n",
    "df_to_vectorize = lemmatized_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630adfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting list of columns to drop\n",
    "columns_to_drop = ['tokenized_text', 'cleaned_text', 'lemmatized_text']\n",
    "\n",
    "# Dropping the specified columns\n",
    "df_to_vectorize.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting head \n",
    "df_to_vectorize.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917c97e",
   "metadata": {},
   "source": [
    "We'll now explore available pre-trained word embedding models in the `gensim-data` repository and download a specific pre-trained word embedding model for our analysis. The process involves:\n",
    "\n",
    "- **Listing Available Models:**\n",
    "  - We print the list of available models in the gensim-data repository using `gensim.downloader.info()['models'].keys()`.\n",
    "\n",
    "- **Downloading the Model:**\n",
    "  - We select the 'fasttext-wiki-news-subwords-300' model and download it using `gensim.downloader.load('fasttext-wiki-news-subwords-300')`. This pre-trained word embedding model is known for its representation of subword information and is well-suited for various natural language processing tasks.\n",
    "\n",
    "Please note that downloading the model may take a while due to its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1cc6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading a pre-trained word embedding model and assigning it to 'model'- this will take a while! \n",
    "model = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a73f3",
   "metadata": {},
   "source": [
    "Let's define a function that generates an embedding for the pre-trained text model by mapping the embeddings into a 300-dimensional space. For out-of-vocabulary words, we use a zero-vector replacement, and we'll remove stop words from the text.\n",
    "- Input: text (str): Text to be embedded.\n",
    "- Output: embedding_vector (np.array): Averaged embedding vector in a 300-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function\n",
    "def text2vec(text):\n",
    "    \n",
    "    tokenized = text\n",
    "    \n",
    "    word_embeddings = [np.zeros(300)]\n",
    "    for word in tokenized:\n",
    "        if word in model:\n",
    "            vector = model[word]\n",
    "        else:\n",
    "            vector = np.zeros(300)\n",
    "            \n",
    "        word_embeddings.append(vector)\n",
    "    \n",
    "    text_embedding = np.stack(word_embeddings).mean(axis=0)\n",
    "    \n",
    "    return text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function over the lemmatized text column and assigning the results to new columns\n",
    "df_to_vectorize['headline_vectors'] = df_to_vectorize['lemmatized_text_no_stopwords'].apply(lambda x: text2vec(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff687bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting headline vectors\n",
    "df_to_vectorize['headline_vectors'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making copy of df_to_vectorize\n",
    "final_df = df_to_vectorize.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'lemmatized_text_no_stopwords' column\n",
    "final_df = final_df.drop('lemmatized_text_no_stopwords', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93383a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking headlines vectors of first row in filtered dataset\n",
    "final_df['headline_vectors'][9207]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365abe1",
   "metadata": {},
   "source": [
    "Let's check for null values and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2452011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking final_df for null values\n",
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all rows containing null values\n",
    "final_df = final_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adceeeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring there are no null values \n",
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995af3cb",
   "metadata": {},
   "source": [
    "To mitigate class imbalance, we will now manually calculate class weights for the 'bias_category' column in our dataset. The process involves:\n",
    "\n",
    "- **Extracting Unique Classes and Counts:**\n",
    "  - We use `np.unique` to extract the unique classes and their corresponding counts from the 'bias_category' column in the DataFrame.\n",
    "\n",
    "- **Calculating Class Weights:**\n",
    "  - The class weights are computed by dividing the total number of samples by the product of the number of classes and the count of each class. This ensures that classes with fewer samples receive higher weights.\n",
    "\n",
    "- **Creating a Class Weight Dictionary:**\n",
    "  - A dictionary, 'class_weight_dict,' is generated by mapping class labels to their respective weights using the `zip` function.\n",
    "\n",
    "- **Displaying Class Weights:**\n",
    "  - The resulting class weights are printed to provide insight into the distribution and importance assigned to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b2f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculating class weights\n",
    "\n",
    "# Extract the unique classes and their counts from the 'bias_category' column in final_df\n",
    "classes, counts = np.unique(final_df['bias_category'], return_counts=True)\n",
    "\n",
    "# Calculate class weights for the 'bias_category' column in final_df\n",
    "total_samples = len(final_df['bias_category'])\n",
    "class_weights = total_samples / (len(classes) * counts)\n",
    "\n",
    "# Create a dictionary mapping class labels to their respective weights\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "# Print the class weights\n",
    "print('Class Weights:', class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68673b29",
   "metadata": {},
   "source": [
    "# Train Test Split<a id='Train_Test_Split'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7858ca",
   "metadata": {},
   "source": [
    "We are now ready to perform a train-test split to prepare our data for modeling. To do this, we import `train_test_split` from `SciKitLearn`. We will then set our X and y variables, split the training and test data, and print the shape for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1afc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X and y variables\n",
    "X = final_df('bias_category', axis=1)\n",
    "y = final_df['bias_category']\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cdff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring there are no NaN values in y_train\n",
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16328845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring there are no NaN values in y_test\n",
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cf98ef",
   "metadata": {},
   "source": [
    "Following the train-test split, we now must convert headline vectors for both the training and testing sets into arrays. The process includes:\n",
    "\n",
    "- **For Training Set (`X_train`):**\n",
    "  - The 'headline_vectors' column in the training set, representing the vectorized form of headlines, is converted to a NumPy array using `np.array` and `tolist()`.\n",
    "\n",
    "- **For Testing Set (`X_test`):**\n",
    "  - Similarly, the 'headline_vectors' column in the testing set undergoes the same conversion to a NumPy array.\n",
    "\n",
    "This conversion facilitates the compatibility of the headline vectors with machine learning models that expect array-like input. The resulting arrays, 'X_train_array' and 'X_test_array,' are now ready for use in model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting X_train vectors to arrays\n",
    "X_train_array = np.array(X_train['headline_vectors'].tolist())\n",
    "\n",
    "# Converting X_test vectors to arrays\n",
    "X_test_array = np.array(X_test['headline_vectors'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc8089",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3230170",
   "metadata": {},
   "source": [
    "### Model Evaluation Metric\n",
    "The **macro average F1 score** is often more appropriate for highly imbalanced datasets such as ours. The macro average calculates the F1 score (a score that considers both precision and recall across different classes) for each class independently, and then takes the unweighted average across all classes. This means that each class contributes equally to the final macro average, regardless of its size.\n",
    "\n",
    "This metric is appropriate for our data, as our target has an imbalanced class distribution. We will use this metric across all models for ease of comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceaac9c",
   "metadata": {},
   "source": [
    "## Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e1858",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic Regression (LogReg) serves as an excellent baseline simple model due to its interpretability, computational efficiency, and ease of implementation. Its linear decision boundary makes it suitable for binary and multiclass classification tasks, providing a straightforward comparison point for more complex models while offering a clear understanding of feature importance in the context of the dataset.\n",
    "\n",
    "We will import all necessary packages and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1fa8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298041f",
   "metadata": {},
   "source": [
    "Next, we instantiate a Logistic Regression model (`LogisticRegression`) configured with class weights derived from our previously calculated `class_weight_dict`. The training process involves:\n",
    "\n",
    "- **Instantiating the Model:**\n",
    "  - We create an instance of Logistic Regression with specified parameters, including a maximum iteration limit (`max_iter=1000`), a random state for reproducibility (`random_state=42`), and class weights determined earlier.\n",
    "\n",
    "- **Model Fitting:**\n",
    "  - The model is fitted to the training data (`X_train_array` and `y_train`) using the `fit` method. During training, the model adjusts its parameters to learn the underlying patterns and relationships..\n",
    "\n",
    "- **Making Predictions:**\n",
    "  - Predictions are generated on the test data (`X_test_array`) using the trained Logistic Regression model. The resulting predictions, stored in `y_pred_lr`, can be evaluated to assess the model's performance on unseen data.\n",
    "\n",
    "Logistic Regression serves as a baseline model, providing a benchmark for more sophisticated algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of LogisticRegression with class weights\n",
    "logreg_model = LogisticRegression(max_iter=1000, random_state=42, class_weight=class_weight_dict)\n",
    "\n",
    "# Fit the model on the training data\n",
    "logreg_model.fit(X_train_array, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_lr = logreg_model.predict(X_test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99875b7",
   "metadata": {},
   "source": [
    "To assess the model, we import `classification_report` and get our evaluation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd18def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ef5d1",
   "metadata": {},
   "source": [
    "**LogReg Model Evaluation:** *Our LogReg model performs reasonably well in identifying low bias headlines but struggles with high and no bias categories.*\n",
    "\n",
    "**High Bias Class:\n",
    "Precision (0.37):** Out of all predicted high bias headlines, 37% were correctly classified.\n",
    "**Recall (0.63):** The model identified 63% of the actual high bias headlines.\n",
    "\n",
    "**Low Bias Class:\n",
    "Precision (0.80):** The model achieved a high accuracy (80%) in predicting low bias headlines.\n",
    "**Recall (0.44):** Only 44% of the actual low bias headlines were correctly identified by the model.\n",
    "\n",
    "**No Bias Class:\n",
    "Precision (0.27):** The model's precision in predicting no bias headlines was relatively low (27%).\n",
    "**Recall (0.59):** The model captured 59% of the actual no bias headlines.\n",
    "\n",
    "Our LogReg Macro Average F1 score is **0.47**, indicating a fair balance between precision and recall across different bias categories without being overly influenced by class size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ad165",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0558a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get unique classes from y_test\n",
    "unique_classes = np.unique(y_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "logreg_cm = confusion_matrix(y_test, y_pred_lr, labels=unique_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(logreg_cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=unique_classes, yticklabels=unique_classes)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6038434",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368971d8",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb25d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the confusion matrix\n",
    "rf_cm = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d713c",
   "metadata": {},
   "source": [
    "## Complex Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e772b6",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_train is a pandas Series with string labels\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encoding y_test and y_train\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# One-hot encoding y_train_encoded and y_test_encoded\n",
    "y_train_one_hot = to_categorical(y_train_encoded)\n",
    "y_test_one_hot = to_categorical(y_test_encoded)\n",
    "\n",
    "# Assuming y_train_encoded is an array of class labels\n",
    "class_labels = np.unique(y_train_encoded)\n",
    "\n",
    "# Compute class weights for the neural network\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=y_train_encoded)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(64, input_dim=X_train_array.shape[1], activation='relu'))\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "nn_model.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "nn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "nn_model.fit(X_train_array, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da342f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the mapping between original class labels and encoded numbers\n",
    "class_labels_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Class Labels Mapping:\", class_labels_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f092efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "nn_model_y_pred_one_hot = nn_model.predict(X_test_array)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "nn_model_y_pred_classes = np.argmax(nn_model_y_pred_one_hot, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_encoded, nn_model_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41fb1a1",
   "metadata": {},
   "source": [
    "neural net with dropout rate to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "nn_model_2 = Sequential()\n",
    "nn_model_2.add(Dense(64, input_dim=X_train_array.shape[1], activation='relu'))\n",
    "nn_model_2.add(Dropout(0.5))  # Adding dropout rate of 0.5\n",
    "nn_model_2.add(Dense(32, activation='relu'))\n",
    "nn_model_2.add(Dropout(0.5))  # Adding dropout rate of 0.5\n",
    "nn_model_2.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "nn_model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "nn_model_2.fit(X_train_array, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "nn_model_2_y_pred_one_hot = nn_model_2.predict(X_test_array)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "nn_model_2_y_pred_classes = np.argmax(nn_model_2_y_pred_one_hot, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_encoded, nn_model_2_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e28063",
   "metadata": {},
   "source": [
    "neural network with l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define the simple neural network model\n",
    "nn_model_3 = Sequential()\n",
    "nn_model_3.add(Dense(64, input_dim=X_train_array.shape[1], activation='relu', kernel_regularizer=l2(0.01)))\n",
    "nn_model_3.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "nn_model_3.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "nn_model_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "nn_model_3.fit(X_train_array, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "nn_model_3_y_pred_one_hot = nn_model_3.predict(X_test_array)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "nn_model_3_y_pred_classes = np.argmax(nn_model_3_y_pred_one_hot, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_encoded, nn_model_3_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1da7ad",
   "metadata": {},
   "source": [
    "add conf matrix here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b076d9a4",
   "metadata": {},
   "source": [
    "neural network with batch normalizatiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced80b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "nn_model_4 = Sequential()\n",
    "nn_model_4.add(Dense(64, input_dim=X_train_array.shape[1], activation='relu'))\n",
    "nn_model_4.add(BatchNormalization())\n",
    "nn_model_4.add(Dense(32, activation='relu'))\n",
    "nn_model_4.add(BatchNormalization())\n",
    "nn_model_4.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "nn_model_4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "nn_model_4.fit(X_train_array, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf44e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "nn_model_4_y_pred_one_hot = nn_model_4.predict(X_test_array)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "nn_model_4_y_pred_classes = np.argmax(nn_model_4_y_pred_one_hot, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_encoded, nn_model_4_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd456b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "nn_model_5 = Sequential()\n",
    "nn_model_5.add(Dense(64, input_dim=X_train_array.shape[1]))\n",
    "nn_model_5.add(LeakyReLU(alpha=0.1))  # Leaky ReLU activation\n",
    "nn_model_5.add(BatchNormalization())\n",
    "nn_model_5.add(Dense(32))\n",
    "nn_model_5.add(LeakyReLU(alpha=0.1))  # Leaky ReLU activation\n",
    "nn_model_5.add(BatchNormalization())\n",
    "nn_model_5.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "nn_model_5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "nn_model_5.fit(X_train_array, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "nn_model_5_y_pred_one_hot = nn_model_5.predict(X_test_array)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "nn_model_5_y_pred_classes = np.argmax(nn_model_5_y_pred_one_hot, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_encoded, nn_model_5_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c4adc",
   "metadata": {},
   "source": [
    "lowering learning rate, adding epochs to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cbfa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "nn_model_6 = Sequential()\n",
    "nn_model_6.add(Dense(64, input_dim=X_train_array.shape[1]))\n",
    "nn_model_6.add(LeakyReLU(alpha=0.1))  # Leaky ReLU activation\n",
    "nn_model_6.add(BatchNormalization())\n",
    "nn_model_6.add(Dense(32))\n",
    "nn_model_6.add(LeakyReLU(alpha=0.1))  # Leaky ReLU activation\n",
    "nn_model_6.add(BatchNormalization())\n",
    "nn_model_6.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with a faster learning rate\n",
    "nn_model_6.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "nn_model_6.fit(X_train_array, y_train_one_hot, epochs=50, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771fc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "nn_model_6_y_pred_one_hot = nn_model_6.predict(X_test_array)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "nn_model_6_y_pred_classes = np.argmax(nn_model_6_y_pred_one_hot, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_encoded, nn_model_6_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0c05f",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4229c2c",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed7dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cc496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input data for GRU (assuming X_train_array has shape (samples, features))\n",
    "X_train_reshaped = X_train_array.reshape((X_train_array.shape[0], X_train_array.shape[1], 1))\n",
    "X_test_reshaped = X_test_array.reshape((X_test_array.shape[0], X_test_array.shape[1], 1))\n",
    "\n",
    "# Define the GRU model with Batch Normalization and Dropout\n",
    "gru_model = Sequential()\n",
    "gru_model.add(GRU(64, input_shape=(X_train_array.shape[1], 1), activation='relu'))\n",
    "gru_model.add(BatchNormalization())\n",
    "gru_model.add(Dropout(0.5))\n",
    "gru_model.add(Dense(32, activation='relu'))\n",
    "gru_model.add(BatchNormalization())\n",
    "gru_model.add(Dropout(0.5))\n",
    "gru_model.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "gru_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True) #setting patience to 1\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    return 0.001 * 0.9 ** epoch\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Train the GRU model with class weights, lowering epochs, increasing batch size\n",
    "gru_model.fit(X_train_reshaped, y_train_one_hot, epochs=5, batch_size=250, validation_split=0.2, class_weight=class_weight_dict, callbacks=[early_stopping, learning_rate_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f02f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred_one_hot_gru = gru_model.predict(X_test_reshaped)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred_classes_gru = np.argmax(y_pred_one_hot_gru, axis=1)\n",
    "\n",
    "# Convert true labels to class labels\n",
    "y_test_classes_gru = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_classes_gru, y_pred_classes_gru, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ac835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "gru_cm = confusion_matrix(y_test_classes_gru, y_pred_classes_gru)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(gru_cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('GRU Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ff9da",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4036ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Define the neural network model with LSTM\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(X_train_array.shape[1], 1), activation='relu'))\n",
    "lstm_model.add(BatchNormalization())\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(32, activation='relu'))\n",
    "lstm_model.add(BatchNormalization())\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights and a faster learning rate\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data for RNN (assuming X_train_array has shape (samples, features))\n",
    "X_train_reshaped = X_train_array.reshape((X_train_array.shape[0], X_train_array.shape[1], 1))\n",
    "\n",
    "# Train the RNN model with class weights\n",
    "lstm_model.fit(X_train_reshaped, y_train_one_hot, epochs=5, batch_size=250, validation_split=0.2, class_weight=class_weight_dict, callbacks=[early_stopping, learning_rate_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ec997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Make predictions on the test data\n",
    "lstm_model_y_pred_one_hot = lstm_model.predict(X_test_reshaped)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "lstm_model_y_pred_classes = np.argmax(lstm_model_y_pred_one_hot, axis=1)\n",
    "\n",
    "# Convert true labels to class labels\n",
    "lstm_model_y_test_classes = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(lstm_model_y_test_classes, lstm_model_y_pred_classes, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "lstm_cm = confusion_matrix(lstm_model_y_test_classes, lstm_model_y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(lstm_cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('RNN Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8bc61c",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a67af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# Define the Bidirectional LSTM model\n",
    "blstm_model = Sequential()\n",
    "blstm_model.add(Bidirectional(LSTM(64, activation='relu'), input_shape=(X_train_array.shape[1], 1)))\n",
    "blstm_model.add(BatchNormalization())\n",
    "blstm_model.add(Dropout(0.5))\n",
    "blstm_model.add(Dense(32, activation='relu'))\n",
    "blstm_model.add(BatchNormalization())\n",
    "blstm_model.add(Dropout(0.5))\n",
    "blstm_model.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model learning rate\n",
    "blstm_model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the Bidirectional LSTM model with class weights\n",
    "blstm_model.fit(X_train_reshaped, y_train_one_hot, epochs=5, batch_size=250, validation_split=0.2, class_weight=class_weight_dict, callbacks=[early_stopping, learning_rate_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca481b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "blstm_model_y_pred_one_hot = blstm_model.predict(X_test_reshaped)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "blstm_y_pred_classes = np.argmax(blstm_model_y_pred_one_hot, axis=1)\n",
    "\n",
    "# Convert true labels to class labels\n",
    "blstm_model_y_test_classes = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(blstm_model_y_test_classes, blstm_y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "blstm_cm = confusion_matrix(blstm_model_y_test_classes, blstm_y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(blstm_cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Bidirectional LSTM Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746adfb0",
   "metadata": {},
   "source": [
    "applying SMOTE to GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb9721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import SMOTE\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a878ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "#Apply SMOTE to the training set\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_array, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e05dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert y_train_smote to a DataFrame\n",
    "y_train_smote_df = pd.DataFrame({'bias_category': y_train_smote})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_smote_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode y_train_smote\n",
    "y_train_smote_one_hot = to_categorical(y_train_smote_df['bias_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71572e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input data for GRU (assuming X_train_array has shape (samples, features))\n",
    "X_train_reshaped = X_train_smote.reshape((X_train_smote.shape[0], X_train_smote.shape[1], 1))\n",
    "X_test_reshaped = X_test_array.reshape((X_test_array.shape[0], X_test_array.shape[1], 1))\n",
    "\n",
    "# Define the GRU model with Batch Normalization and Dropout\n",
    "gru_model_smote = Sequential()\n",
    "gru_model_smote.add(GRU(64, input_shape=(X_train_smote.shape[1], 1), activation='relu'))\n",
    "gru_model_smote.add(BatchNormalization())\n",
    "gru_model_smote.add(Dropout(0.5))\n",
    "gru_model_smote.add(Dense(32, activation='relu'))\n",
    "gru_model_smote.add(BatchNormalization())\n",
    "gru_model_smote.add(Dropout(0.5))\n",
    "gru_model_smote.add(Dense(len(class_labels), activation='softmax'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "gru_model_smote.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)  # setting patience to 1\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    return 0.001 * 0.9 ** epoch\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Train the GRU model with class weights, lowering epochs, increasing batch size\n",
    "gru_model_smote.fit(X_train_reshaped, y_train_smote_one_hot, epochs=5, batch_size=250, validation_split=0.2, class_weight=class_weight_dict, callbacks=[early_stopping, learning_rate_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2be003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred_one_hot_gru_smote = gru_model_smote.predict(X_test_reshaped)\n",
    "\n",
    "# Convert the predicted probabilities to class labels\n",
    "y_pred_classes_gru_smote = np.argmax(y_pred_one_hot_gru_smote, axis=1)\n",
    "\n",
    "# Convert true labels to class labels\n",
    "y_test_classes_gru_smote = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_classes_gru_smote, y_pred_classes_gru_smote, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b85f7",
   "metadata": {},
   "source": [
    "print all macro averaged scores for the best performing models (rf, nn, log reg)\n",
    "print feature importances for each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519ed9bb",
   "metadata": {},
   "source": [
    "Future Aspirations: A Tool for Writers\n",
    "\n",
    "Looking ahead, a pivotal goal is to create a model or web app empowering writers to assess their headlines' predicted bias scores before publication. This not only emphasizes the commitment to unbiased reporting but also provides a practical solution for writers to navigate the nuanced landscape of headline neutrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27688f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
